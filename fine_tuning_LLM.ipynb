{"cells":[{"cell_type":"markdown","metadata":{"id":"1Yqw8OPtsKGJ"},"source":["# Requirements"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 datasets torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OaYaZ7vDkYDO"},"outputs":[],"source":["import os\n","import torch\n","# torch.cuda.empty_cache()\n","from datasets import load_dataset\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    HfArgumentParser,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from peft import LoraConfig, PeftModel\n","from trl import SFTTrainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tP7Yar12kcFs"},"outputs":[],"source":["################################################################################\n","# QLoRA parameters\n","################################################################################\n","\n","# # LoRA attention dimension\n","# lora_r = 64\n","\n","# # Alpha parameter for LoRA scaling\n","# lora_alpha = 16\n","\n","# # Dropout probability for LoRA layers\n","# lora_dropout = 0.1\n","\n","################################################################################\n","# bitsandbytes parameters\n","################################################################################\n","\n","# # Activate 4-bit precision base model loading\n","# use_4bit = True\n","\n","# # Compute dtype for 4-bit base models\n","# bnb_4bit_compute_dtype = \"float16\"\n","\n","# # Quantization type (fp4 or nf4)\n","# bnb_4bit_quant_type = \"nf4\"\n","\n","# # Activate nested quantization for 4-bit base models (double quantization)\n","# use_nested_quant = False\n","\n","################################################################################\n","# TrainingArguments parameters\n","################################################################################\n","\n","# # Output directory where the model predictions and checkpoints will be stored\n","# output_dir = \"./results\"\n","\n","# # Number of training epochs\n","# num_train_epochs = 100\n","\n","# # Enable fp16/bf16 training (set bf16 to True with an A100)\n","# fp16 = False\n","# bf16 = False\n","\n","# # Batch size per GPU for training\n","# per_device_train_batch_size = 4\n","\n","# # Batch size per GPU for evaluation\n","# per_device_eval_batch_size = 4\n","\n","# # Number of update steps to accumulate the gradients for\n","# gradient_accumulation_steps = 1\n","\n","# # Enable gradient checkpointing\n","# gradient_checkpointing = True\n","\n","# # Maximum gradient normal (gradient clipping)\n","# max_grad_norm = 0.3\n","\n","# # Initial learning rate (AdamW optimizer)\n","# learning_rate = 2e-4\n","\n","# # Weight decay to apply to all layers except bias/LayerNorm weights\n","# weight_decay = 0.001\n","\n","# # Optimizer to use\n","# optim = \"paged_adamw_32bit\"\n","\n","# # Learning rate schedule (constant a bit better than cosine)\n","# lr_scheduler_type = \"constant\"\n","\n","# # Number of training steps (overrides num_train_epochs)\n","# max_steps = -1\n","\n","# # Ratio of steps for a linear warmup (from 0 to learning rate)\n","# warmup_ratio = 0.03\n","\n","# # Group sequences into batches with same length\n","# # Saves memory and speeds up training considerably\n","# group_by_length = True\n","\n","# # Save checkpoint every X updates steps\n","# save_steps = 25\n","\n","# # Log every X updates steps\n","# logging_steps = 25\n","\n","################################################################################\n","# SFT parameters\n","################################################################################\n","\n","# # Maximum sequence length to use\n","# max_seq_length = None\n","\n","# # Pack multiple short examples in the same input sequence to increase efficiency\n","# packing = False"]},{"cell_type":"markdown","metadata":{"id":"qt-KLD4brylv"},"source":["# Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rXPq6je4nSu8"},"outputs":[],"source":["dataset_name = \"mlabonne/guanaco-llama2-1k\"\n","dataset = load_dataset(dataset_name, split=\"train\")"]},{"cell_type":"markdown","metadata":{"id":"H4J_uCc_r0eD"},"source":["# Quantization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r7PXRIH6nZYn"},"outputs":[],"source":["compute_dtype = getattr(torch, \"float16\")\n","use_4bit = True\n","\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=use_4bit,                    # Activate 4-bit precision base model loading\n","    bnb_4bit_quant_type=\"nf4\",            # Quantization type (fp4 or nf4)\n","    bnb_4bit_compute_dtype=compute_dtype, # Compute dtype for 4-bit base models\n","    bnb_4bit_use_double_quant=False,      # Activate nested quantization for 4-bit base models (double quantization)\n",")\n","\n","# Check GPU compatibility with bfloat16\n","if compute_dtype == torch.float16 and use_4bit:\n","    major, _ = torch.cuda.get_device_capability()\n","    if major >= 8:\n","        print(\"=\" * 80)\n","        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n","        print(\"=\" * 80)"]},{"cell_type":"markdown","metadata":{"id":"mDFe8bsor3AK"},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OX4C-USkkg-h"},"outputs":[],"source":["# Load base model\n","model_name = \"NousResearch/llama-2-7b-chat-hf\"\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    quantization_config=bnb_config,\n","    device_map= {\"\": 0}               # Load the entire model on the GPU 0\n",")\n","\n","# Model configs\n","model.config.use_cache = False\n","model.config.pretraining_tp = 1"]},{"cell_type":"markdown","metadata":{"id":"vFpMuNrbr4tS"},"source":["# Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I0Lu4m-bneeK"},"outputs":[],"source":["# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","\n","# Tokenizer configs\n","tokenizer.pad_token = tokenizer.eos_token\n","tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training"]},{"cell_type":"markdown","metadata":{"id":"E3YALh_Pr6ZE"},"source":["# PEFT & LORA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eMtzNdQXotqE"},"outputs":[],"source":["# Load LoRA configuration\n","peft_config = LoraConfig(\n","    lora_alpha=16,            # Alpha parameter for LoRA scaling\n","    lora_dropout=0.1,         # Dropout probability for LoRA layers\n","    r=64,                     # LoRA attention dimension\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")"]},{"cell_type":"markdown","metadata":{"id":"u5EbIkWIr_7H"},"source":["# Fine-Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ea50K1cLlDfc"},"outputs":[],"source":["# Set training parameters\n","training_arguments = TrainingArguments(\n","    output_dir=\"./results\",           # Output directory where the model predictions and checkpoints will be stored\n","    num_train_epochs=100,             # Number of training epochs\n","    per_device_train_batch_size=4,    # Batch size per GPU for training\n","    gradient_accumulation_steps=1,    # Number of update steps to accumulate the gradients for\n","    optim=\"paged_adamw_32bit\",        # Optimizer to use\n","    save_steps=25,                    # Save checkpoint every X updates steps\n","    logging_steps=25,                 # Log every X updates steps\n","    learning_rate=2e-4,               # Initial learning rate (AdamW optimizer)\n","    weight_decay=0.001,               # Weight decay to apply to all layers except bias/LayerNorm weights\n","    bf16=False,                       # Enable fp16/bf16 training (set bf16 to True with an A100)\n","    fp16=False,\n","    max_grad_norm=0.3,                # Maximum gradient normal (gradient clipping)\n","    max_steps=-1,                     # Number of training steps (overrides num_train_epochs)\n","    warmup_ratio=0.03,                # Ratio of steps for a linear warmup (from 0 to learning rate)\n","    group_by_length=True,             # Group sequences into batches with same length (Saves memory and speeds up training considerably)\n","    lr_scheduler_type=\"constant\",     # Learning rate schedule (constant a bit better than cosine)\n","    report_to=\"tensorboard\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aMaHcYWGnK07"},"outputs":[],"source":["# Set supervised fine-tuning parameters\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=dataset,\n","    peft_config=peft_config,\n","    dataset_text_field=\"text\",\n","    max_seq_length=None,          # Maximum sequence length to use\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing=False,                # Pack multiple short examples in the same input sequence to increase efficiency\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"sAbX5PzFnNVN","outputId":"e6049f53-a228-474a-91b3-3a34d691c1ab"},"outputs":[{"name":"stderr","output_type":"stream","text":["You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='805' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [  805/25000 1:21:30 < 40:55:55, 0.16 it/s, Epoch 3.22/100]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>25</td>\n","      <td>1.346700</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>1.611400</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>1.207600</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>1.433800</td>\n","    </tr>\n","    <tr>\n","      <td>125</td>\n","      <td>1.178000</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>1.356300</td>\n","    </tr>\n","    <tr>\n","      <td>175</td>\n","      <td>1.171000</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>1.452900</td>\n","    </tr>\n","    <tr>\n","      <td>225</td>\n","      <td>1.153800</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>1.521800</td>\n","    </tr>\n","    <tr>\n","      <td>275</td>\n","      <td>1.115600</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>1.338400</td>\n","    </tr>\n","    <tr>\n","      <td>325</td>\n","      <td>1.136600</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>1.347100</td>\n","    </tr>\n","    <tr>\n","      <td>375</td>\n","      <td>1.132500</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>1.257800</td>\n","    </tr>\n","    <tr>\n","      <td>425</td>\n","      <td>1.103400</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>1.389100</td>\n","    </tr>\n","    <tr>\n","      <td>475</td>\n","      <td>1.103800</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>1.332500</td>\n","    </tr>\n","    <tr>\n","      <td>525</td>\n","      <td>1.095500</td>\n","    </tr>\n","    <tr>\n","      <td>550</td>\n","      <td>1.165900</td>\n","    </tr>\n","    <tr>\n","      <td>575</td>\n","      <td>1.105300</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>1.087400</td>\n","    </tr>\n","    <tr>\n","      <td>625</td>\n","      <td>0.999800</td>\n","    </tr>\n","    <tr>\n","      <td>650</td>\n","      <td>1.270800</td>\n","    </tr>\n","    <tr>\n","      <td>675</td>\n","      <td>1.055900</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>1.203400</td>\n","    </tr>\n","    <tr>\n","      <td>725</td>\n","      <td>1.119200</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>1.155600</td>\n","    </tr>\n","    <tr>\n","      <td>775</td>\n","      <td>0.965300</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>1.009200</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n","  warnings.warn(\n"]}],"source":["# Train model\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e-uJ3184nRac"},"outputs":[],"source":["# Save trained model\n","new_model = \"llama-2-7b-miniguanaco\"\n","trainer.model.save_pretrained(new_model)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
